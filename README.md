# AnimalTaskSim

AnimalTaskSim is a **benchmark** for reproducing classic animal behavioral tasks in silico and scoring AI agents against **animal-style fingerprints** â€” not just reward rate. It ships **task-faithful environments**, **baseline agents**, and an **evaluation suite** that compares agents to mice/primate benchmarks on psychometric/chronometric curves, history effects, and training dynamics.

> Scope for v0.1 (MVP): two tasks
>
> 1) **Mouse 2AFC visual decision (IBL-style)**  
> 2) **Macaque random-dot motion discrimination (RDM)**

We aim to make "replication with teeth" the standard: codify the task, train the agent under the same constraints, then grade it with the *same* metrics labs use.

---

## ðŸ“Š Results & Findings

**TL;DR:** We achieved excellent matches on some metrics (99% bias accuracy, 100% win-stay rate) but completely failed others (RT dynamics). RL agents optimize reward too efficiently, finding shortcuts that animals don't use.

**Read the full analysis:** [**FINDINGS.md**](FINDINGS.md)

**View interactive dashboards:**
- [Mouse 2AFC (Sticky-GLM v21)](runs/ibl_final_dashboard.html) - 99% bias match, 79% history match
- [Macaque RDM (PPO v24)](runs/rdm_final_dashboard.html) - 100% win-stay match, RT dynamics failed
- [Macaque RDM (DDM v2)](runs/rdm_ddm_dashboard.html) - Best RT dynamics (81% intercept match)

**Key insight:** Behavioral replication requires architectural constraints (like DDM's evidence accumulation), not just reward shaping. Future work: hybrid DDM+LSTM architecture.

---

## Why this exists

- Toolkits like NeuroGym/PsychRNN and virtual labs like Animal-AI are valuable, but none combine **task-faithful environments + public animal data + fingerprint metrics + a reproducible scoring pipeline**.
- Labs and RL teams need a **defensible yardstick** for â€œanimal-like behavior.â€ Reward rate alone is vanity; fingerprints are substance.

---

## Repo structure (planned)

```
animal-task-sim/
â”œâ”€ envs/
â”‚  â”œâ”€ ibl_2afc.py            # Gymnasium env for mouse 2AFC
â”‚  â”œâ”€ rdm_macaque.py         # Gymnasium env for RDM
â”‚  â””â”€ utils_timing.py        # Common trial/timing utilities
â”œâ”€ agents/
â”‚  â”œâ”€ sticky_q.py            # Q-learning + stickiness baseline
â”‚  â”œâ”€ bayes_observer.py      # Ideal observer with sensory noise
â”‚  â””â”€ ppo_baseline.py        # Stable-Baselines3 PPO wrapper
â”œâ”€ eval/
â”‚  â”œâ”€ metrics.py             # Psychometric/chronometric, history kernels
â”‚  â”œâ”€ fitters.py             # Logistic fits, RT fits, kernel regressions
â”‚  â”œâ”€ report.py              # End-to-end replication report generation
â”‚  â””â”€ simlog_schema.json     # Unified trial-log schema
â”œâ”€ data/
â”‚  â”œâ”€ ibl/
â”‚  â”‚  â””â”€ download_ibl.py     # Script to fetch example IBL behavior slices
â”‚  â””â”€ macaque/
â”‚     â””â”€ references.md       # Protocol references & expected ranges
â”œâ”€ scripts/
â”‚  â”œâ”€ train_agent.py         # Train any agent on any env via CLI
â”‚  â”œâ”€ evaluate_agent.py      # Run eval suite and save report
â”‚  â””â”€ make_report.py         # Pretty HTML/PDF report
â”œâ”€ notebooks/
â”‚  â”œâ”€ 00_quickstart.ipynb
â”‚  â””â”€ 10_recreate_figures.ipynb
â”œâ”€ tests/
â”‚  â”œâ”€ test_envs.py
â”‚  â”œâ”€ test_metrics.py
â”‚  â””â”€ test_agents.py
â”œâ”€ pyproject.toml
â”œâ”€ README.md
â””â”€ LICENSE
```

---

## Task specs (v0.1)

### 1) Mouse visual 2AFC (IBL-style)

**Goal:** choose left vs right based on signed contrast. Sessions mix **block priors** (e.g., 80/20 left/right) and **neutral** blocks.  
**Trial phases:** ITI â†’ fixation (optional) â†’ stimulus (contrast) â†’ response window â†’ outcome (reward/timeout).

**Observation (default):**
- `contrast`: float in [-1,1] (sign = side; magnitude = difficulty)
- `phase`: one-hot of trial phase (optional)
- `t_in_phase`: normalized time within phase (optional)

> Note: block prior is **not** exposed to the agent by default (mirrors animals). A diagnostic flag `expose_prior` can be toggled for ablations.

**Action space:** `Discrete(3)` â†’ {`left`, `right`, `no-op`}  
**Reward:** `+1` on correct choice; `0` otherwise; configurable timeouts / ITI penalties.  
**Episode:** fixed-length session (e.g., 400 trials) â†’ `done=True`.

**Key parameters:**
- Contrast set: e.g., `{Â±0, 6.25, 12.5, 25, 50, 100}%` mapped to [-1,1]
- Block structure: sequences of biased vs neutral blocks
- ITI, response window, timeouts (all in steps or seconds â†’ discretized)

### 2) Macaque random-dot motion (RDM)

**Goal:** decide left vs right based on motion direction at coherence `c`.  
**Trial phases:** fixation â†’ stimulus (dots) â†’ (optional) go cue â†’ response â†’ outcome.

**Observation (default):**
- `coherence`: float in [0,1] (signed by direction)
- `phase` and `t_in_phase` as above (optional)

**Action space:** `Discrete(3)` â†’ {`left`, `right`, `hold`}  
**Reward:** `+1` on correct; optional small cost per time-step to incentivize speedâ€“accuracy tradeoff.  
**Timing:** max stimulus duration; optional collapsing decision bound implemented as reward shaping or termination rule.

---

## Unified trial-log schema

Every run writes newline-delimited JSON (`.ndjson`) with the following fields per trial:

```json
{
  "task": "ibl_2afc|rdm",
  "session_id": "uuid",
  "trial_index": 42,
  "stimulus": {"contrast": 0.25, "side": "right"},
  "block_prior": {"p_right": 0.8},
  "action": "left",
  "correct": false,
  "reward": 0.0,
  "rt_ms": 540,
  "phase_times": {"stim_ms": 300, "resp_ms": 700},
  "prev": {"action": "right", "reward": 1},
  "seed": 1234,
  "agent": {"name": "sticky_q", "version": "0.1.0"}
}
```

This fuels the evaluation suite independent of the agent/env internals.

---

## Evaluation metrics (v0.1)

- **Psychometric curve**: P(choice=right) vs signed contrast; fit logistic â†’ report **threshold**, **slope**, **lapse**.
- **Chronometric curve** (RDM): median RT vs coherence; report **speedâ€“accuracy tradeoff** stats.
- **History fingerprints**: win-stay/lose-shift rates; **sticky-choice**; logistic **history kernel regression** (past K choices/outcomes).
- **Bias & priors**: block-prior effect size on choice bias.
- **Training dynamics**: trials-to-criterion; early vs late session performance deltas.

Output: JSON metrics + an HTML report that recreates two canonical figures per task (psychometric + history; plus chronometric for RDM).

---

## Baseline agents (v0.1)

1) **Sticky-Q**: tabular Q-learning with a **stickiness term** in the choice rule.  
2) **Bayesian observer**: ideal observer with sensory noise; softmax decision with lapses.  
3) **PPO baseline**: Stable-Baselines3 PPO with masked actions during non-response phases.

Each baseline must:
- Save seeds & configs
- Log to `.ndjson` schema
- Train â‰¤ 20 minutes on CPU for a demo run (small configs)
- Produce a valid evaluation report via `scripts/evaluate_agent.py`

---

## Installation (dev)

```bash
pyenv local 3.11.9         # or your preferred Python 3.11
python -m venv .venv && source .venv/bin/activate
pip install -e .[dev]       # via pyproject optional extras
```

**Key deps:** `gymnasium`, `numpy`, `scipy`, `pandas`, `torch`, `stable-baselines3`, `matplotlib`, `tyro` (or `argparse`), `pydantic` for config, `pytest`.

---

## CLI

```bash
# Install
pip install -e ".[dev]"

# Train Sticky-GLM on IBL 2AFC (writes runs/ibl_stickyq)
python scripts/train_agent.py --env ibl_2afc --agent sticky_q --steps 2000 --out runs/ibl_stickyq \
  --sticky_q.learning-rate 0.05 --sticky_q.temperature 1.0

# Train Bayesian observer on RDM
python scripts/train_agent.py --env rdm --agent bayes --steps 1200 --out runs/rdm_bayes

# Train PPO baseline on RDM with streaming evidence & per-step cost
python scripts/train_agent.py --env rdm --agent ppo --steps 60000 --trials-per-episode 600 \
  --ppo.per-step-cost 0.02 --ppo.evidence-gain 0.1 --ppo.momentary-sigma 3.0 --out runs/rdm_ppo

# Evaluate (writes metrics.json)
python scripts/evaluate_agent.py --run runs/ibl_stickyq

# Generate HTML report
python scripts/make_report.py --run runs/ibl_stickyq

# Optional overrides
python scripts/evaluate_agent.py --log some_agent.ndjson --out reports/some_agent_metrics.json
python scripts/make_report.py --run runs/ibl_stickyq --log runs/ibl_stickyq/trials.ndjson --metrics runs/ibl_stickyq/metrics.json --out reports/custom.html
```

## Running baselines & comparing to reference data

1. Train an agent with `scripts/train_agent.py` to populate `runs/<name>/` with `trials.ndjson` and configs.
2. Evaluate that log (`scripts/evaluate_agent.py --run <run_dir>`) to produce `metrics.json`.
3. Generate an HTML report (`scripts/make_report.py --run <run_dir>`) to visualise psychometric/chronometric/history fingerprints alongside the metrics JSON.
4. Run the same evaluation on reference `.ndjson` logs under `data/ibl/` or `data/macaque/`, then compare the resulting metrics JSON to your agent (e.g. via a notebook or a short Python diff script).
5. Iterate on hyperparameters (`--sticky_q.stickiness`, `--bayes.sensory_sigma`, `--ppo.per-step-cost`, etc.) until the fingerprints align with target animal ranges.

### Calibration helper (recommended workflow)

You can automate the training â†’ evaluate â†’ report loop using the calibration script:

```bash
# Run GLM, PPO, and DDM baselines with default settings
python scripts/calibration.py --mode all

# Sticky-GLM only (adjust hyperparameters inline)
python scripts/calibration.py --mode sticky \
  --sticky.output runs/ibl_stickyq_calib_tuned \
  --sticky.episodes 20 \
  --sticky.learning-rate 0.03 \
  --sticky.weight-decay 0.001 \
  --sticky.temperature 1.1

# PPO (RDM) only with streaming evidence and collapsing bounds
python scripts/calibration.py --mode ppo \
  --ppo.output runs/rdm_ppo_calib_experiment \
  --ppo.total-timesteps 180000 \
  --ppo.trials-per-episode 900 \
  --ppo.per-step-cost 0.025 \
  --ppo.evidence-gain 0.01 \
  --ppo.momentary-sigma 5.0 \
  --ppo.bound-threshold 3.5

# Analytical DDM baseline for the RDM task
python scripts/calibration.py --mode ddm \
  --ddm.output runs/rdm_ddm_baseline \
  --ddm.trials-per-episode 600 \
  --ddm.bound 12.0 \
  --ddm.drift-gain 0.1
```

Each run writes `metrics.json` and `report.html` into the chosen `runs/<name>/` directory so you can compare fingerprints quickly.

---

## Milestones (time-boxed)

- **Week 1â€“2:** envs + Sticky-Q + metrics (psychometric, history) â†’ first replication report for IBL-style task. **Gate:** curves look sane.  
- **Week 3â€“4:** PPO + Bayesian observer + RDM env + chronometric metrics â†’ second report. **Gate:** PPO nails reward rate; mismatches at least one fingerprint; Bayesian moves it closer.

---

## Principles

- **Fidelity over flash:** copy trial structure and timing; keep visuals minimal.
- **Fingerprints over reward:** we celebrate matched **statistics**, not points scored.
- **Reproducibility:** fixed seeds, saved configs, exact software versions.
- **Separation of concerns:** envs, agents, and metrics do not depend on each other.

---

## Roadmap

**v0.1 (MVP)** â€” IBL 2AFC + Macaque RDM; baselines (Stickyâ€‘Q, Bayesian observer, PPO); evaluation suite & HTML reports.

**v0.2 (next)** â€” cognitive axes that reuse the same scaffold:
- **Mouse PRL (flexibility):** reversal blocks with p=0.8/0.2; metrics include trialsâ€‘toâ€‘criterion, perseverative/regressive errors, WS/LS asymmetry.
- **Macaque DMS (working memory):** sampleâ†’delayâ†’test; metrics include accuracy vs delay, RT vs delay, lureâ€‘specific errors.

> CLI (planned) examples:
> ```bash
> # PRL (as a mode of ibl_2afc)
> python scripts/train_agent.py --env ibl_2afc --agent sticky_q --steps 15000 --out runs/prl_stickyq --reversal true
> 
> # DMS
> python scripts/train_agent.py --env dms --agent ppo --steps 30000 --out runs/dms_ppo --delays 250,500,1000,2000,3000
> ```

---

## Ethics note

This does **not** claim to replace animal research. Itâ€™s a comparative benchmark that pressures AI to behave under animal-like constraints and makes replication transparent and reusable.

---

## License

MIT for code; dataset licenses remain with their owners.
